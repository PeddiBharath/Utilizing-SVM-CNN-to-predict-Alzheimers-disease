{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e004ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "125/125 [==============================] - 2s 5ms/step - loss: 58.7788 - accuracy: 0.9619 - val_loss: 109.1435 - val_accuracy: 0.9707\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 38.9441 - accuracy: 0.9656 - val_loss: 104.0280 - val_accuracy: 0.9707\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 22.2862 - accuracy: 0.9649 - val_loss: 100.9717 - val_accuracy: 0.9707\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 28.2197 - accuracy: 0.9716 - val_loss: 8.5544 - val_accuracy: 0.9684\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 23.1054 - accuracy: 0.9629 - val_loss: 28.9282 - val_accuracy: 0.9707\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 28.7291 - accuracy: 0.9729 - val_loss: 14.5803 - val_accuracy: 0.9684\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 13.5811 - accuracy: 0.9649 - val_loss: 71.4238 - val_accuracy: 0.9707\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 37.0011 - accuracy: 0.9681 - val_loss: 80.6794 - val_accuracy: 0.9707\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 27.2717 - accuracy: 0.9681 - val_loss: 11.6665 - val_accuracy: 0.9684\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 16.3884 - accuracy: 0.9666 - val_loss: 48.3851 - val_accuracy: 0.9707\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 10.8088 - accuracy: 0.9616 - val_loss: 74.5031 - val_accuracy: 0.9707\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 21.8281 - accuracy: 0.9706 - val_loss: 59.2743 - val_accuracy: 0.9707\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 18.9135 - accuracy: 0.9737 - val_loss: 1.6086 - val_accuracy: 0.9300\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 10.0748 - accuracy: 0.9674 - val_loss: 2.9554 - val_accuracy: 0.9661\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 14.0479 - accuracy: 0.9684 - val_loss: 13.0283 - val_accuracy: 0.9684\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 7.1784 - accuracy: 0.9684 - val_loss: 8.8731 - val_accuracy: 0.9707\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 6.7131 - accuracy: 0.9686 - val_loss: 2.4874 - val_accuracy: 0.9707\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 8.3280 - accuracy: 0.9671 - val_loss: 24.6968 - val_accuracy: 0.9707\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 4.4542 - accuracy: 0.9721 - val_loss: 7.0162 - val_accuracy: 0.9707\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 5.9414 - accuracy: 0.9659 - val_loss: 30.1529 - val_accuracy: 0.9707\n",
      "47/47 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "df1 = pd.read_csv('class_labels.csv')\n",
    "df2 = pd.read_csv('go_features.csv')\n",
    "df3 = pd.read_csv('gtex_features.csv')\n",
    "df4 = pd.read_csv('kegg_features.csv')\n",
    "df5 = pd.read_csv('pathdipall_features.csv')\n",
    "df6 = pd.read_csv('ppi_features.csv')\n",
    "\n",
    "merged_df = pd.merge(df1, df2, on='entrezId')\n",
    "merged_df = pd.merge(merged_df, df3, on='entrezId')\n",
    "merged_df = pd.merge(merged_df, df4, on='entrezId')\n",
    "merged_df = pd.merge(merged_df, df5, on='entrezId')\n",
    "merged_df = pd.merge(merged_df, df6, on='entrezId')\n",
    "\n",
    "merged_df.fillna(0, inplace=True)\n",
    "merged_df.to_csv('merged_dataset.csv', index=False)\n",
    "\n",
    "dataset = pd.read_csv('merged_dataset.csv')\n",
    "X = dataset.iloc[:, [0] + list(range(2, dataset.shape[1]))].values\n",
    "Y = dataset.iloc[:, 1].values\n",
    "\n",
    "# Feature extraction using PCA\n",
    "num_components = 16  # Adjust the number of PCA components as needed\n",
    "pca = PCA(n_components=num_components)\n",
    "X_transformed = pca.fit_transform(X)\n",
    "\n",
    "# Reshape the transformed data into images (assuming each row has the same number of components)\n",
    "num_rows, num_components = X_transformed.shape\n",
    "num_pixels = int(np.sqrt(num_components))  # Assuming the data can be represented as square images\n",
    "X_images = X_transformed.reshape(-1, num_pixels, num_pixels, 1)  # Reshape to (num_rows, num_pixels, num_pixels, 1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(np.unique(Y))\n",
    "Y_onehot = tf.keras.utils.to_categorical(Y, num_classes)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_images, Y_onehot, test_size=0.25, random_state=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(num_pixels, num_pixels, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 20  # Set the number of epochs here\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "Y_pred_onehot = model.predict(X_test)\n",
    "Y_pred_labels = np.argmax(Y_pred_onehot, axis=1)  # Convert one-hot predictions to class labels\n",
    "Y_test_labels = np.argmax(Y_test, axis=1)  # Convert one-hot ground truth to class labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2476a0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.71%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(Y_test_labels, Y_pred_labels)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0cde793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 249ms/step\n",
      "[False]\n"
     ]
    }
   ],
   "source": [
    "#Testing with new data\n",
    "#entrezId = 5728 means no alzheimer lets see if get the desired output\n",
    "\n",
    "merged_new_data = pd.read_csv('Test.csv')\n",
    "specified_entrezId = 5728\n",
    "selected_row = merged_new_data.query(\"entrezId == @specified_entrezId\")\n",
    "# Extract features using PCA (using the same PCA object from the training step)\n",
    "X_new = selected_row.iloc[:, [0] + list(range(2, selected_row.shape[1]))].values\n",
    "X_transformed_new = pca.transform(X_new)\n",
    "\n",
    "# Reshape the transformed data into images\n",
    "X_images_new = X_transformed_new.reshape(-1, num_pixels, num_pixels, 1)\n",
    "\n",
    "# Assuming the model is already trained and loaded, as shown in your code\n",
    "# Make predictions on the new data\n",
    "Y_pred_onehot_new = model.predict(X_images_new)\n",
    "Y_pred_labels_new = np.argmax(Y_pred_onehot_new, axis=1)  # Convert one-hot predictions to class labels\n",
    "\n",
    "# Assuming '1' is the class label for Alzheimer's and '0' is the class label for non-Alzheimer's\n",
    "# You can use the following line to get a boolean array indicating whether each sample is predicted to have Alzheimer's or not\n",
    "predicted_has_alzheimer = Y_pred_labels_new == 1\n",
    "\n",
    "# Print the prediction for each sample in the new data\n",
    "print(predicted_has_alzheimer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
