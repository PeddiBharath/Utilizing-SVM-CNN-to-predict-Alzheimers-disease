{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78e004ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "125/125 [==============================] - 1s 4ms/step - loss: 264.4175 - accuracy: 0.9533 - val_loss: 196.1052 - val_accuracy: 0.9707\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 68.2517 - accuracy: 0.9611 - val_loss: 157.1378 - val_accuracy: 0.9707\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 90.7179 - accuracy: 0.9691 - val_loss: 168.0536 - val_accuracy: 0.9707\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 66.1977 - accuracy: 0.9694 - val_loss: 24.6512 - val_accuracy: 0.9707\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 59.5340 - accuracy: 0.9619 - val_loss: 203.7991 - val_accuracy: 0.9707\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 62.5562 - accuracy: 0.9691 - val_loss: 127.3104 - val_accuracy: 0.9707\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 40.0590 - accuracy: 0.9669 - val_loss: 126.4707 - val_accuracy: 0.9707\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 40.3317 - accuracy: 0.9696 - val_loss: 42.9561 - val_accuracy: 0.9707\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 27.8733 - accuracy: 0.9669 - val_loss: 73.3900 - val_accuracy: 0.9707\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 25.7041 - accuracy: 0.9634 - val_loss: 89.7908 - val_accuracy: 0.9707\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 39.1395 - accuracy: 0.9691 - val_loss: 99.0654 - val_accuracy: 0.9707\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 36.6544 - accuracy: 0.9696 - val_loss: 48.0872 - val_accuracy: 0.9707\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 29.4129 - accuracy: 0.9636 - val_loss: 82.8353 - val_accuracy: 0.9707\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 19.1940 - accuracy: 0.9711 - val_loss: 35.4693 - val_accuracy: 0.9707\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 21.5046 - accuracy: 0.9664 - val_loss: 8.2883 - val_accuracy: 0.9707\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 16.3842 - accuracy: 0.9671 - val_loss: 55.1021 - val_accuracy: 0.9707\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 21.8658 - accuracy: 0.9679 - val_loss: 23.2191 - val_accuracy: 0.9707\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 18.4619 - accuracy: 0.9691 - val_loss: 30.3966 - val_accuracy: 0.9707\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 11.2634 - accuracy: 0.9639 - val_loss: 22.4389 - val_accuracy: 0.9707\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 0s 2ms/step - loss: 13.8582 - accuracy: 0.9749 - val_loss: 4.8622 - val_accuracy: 0.9707\n",
      "47/47 [==============================] - 0s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "df1 = pd.read_csv('class_labels.csv')\n",
    "df2 = pd.read_csv('go_features.csv')\n",
    "df3 = pd.read_csv('gtex_features.csv')\n",
    "df4 = pd.read_csv('kegg_features.csv')\n",
    "df5 = pd.read_csv('pathdipall_features.csv')\n",
    "df6 = pd.read_csv('ppi_features.csv')\n",
    "\n",
    "merged_df = pd.merge(df1, df2, on='entrezId')\n",
    "merged_df = pd.merge(merged_df, df3, on='entrezId')\n",
    "merged_df = pd.merge(merged_df, df4, on='entrezId')\n",
    "merged_df = pd.merge(merged_df, df5, on='entrezId')\n",
    "merged_df = pd.merge(merged_df, df6, on='entrezId')\n",
    "\n",
    "merged_df.fillna(0, inplace=True)\n",
    "merged_df.to_csv('merged_dataset.csv', index=False)\n",
    "\n",
    "dataset = pd.read_csv('merged_dataset.csv')\n",
    "X = dataset.iloc[:, [0] + list(range(2, dataset.shape[1]))].values\n",
    "Y = dataset.iloc[:, 1].values\n",
    "\n",
    "# Feature extraction using PCA\n",
    "num_components = 16  # Adjust the number of PCA components as needed\n",
    "pca = PCA(n_components=num_components)\n",
    "X_transformed = pca.fit_transform(X)\n",
    "\n",
    "# Reshape the transformed data into images (assuming each row has the same number of components)\n",
    "num_rows, num_components = X_transformed.shape\n",
    "num_pixels = int(np.sqrt(num_components))  # Assuming the data can be represented as square images\n",
    "X_images = X_transformed.reshape(-1, num_pixels, num_pixels, 1)  # Reshape to (num_rows, num_pixels, num_pixels, 1)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(np.unique(Y))\n",
    "Y_onehot = tf.keras.utils.to_categorical(Y, num_classes)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_images, Y_onehot, test_size=0.25, random_state=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(num_pixels, num_pixels, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "batch_size = 32\n",
    "epochs = 20  # Set the number of epochs here\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "Y_pred_onehot = model.predict(X_test)\n",
    "Y_pred_labels = np.argmax(Y_pred_onehot, axis=1)  # Convert one-hot predictions to class labels\n",
    "Y_test_labels = np.argmax(Y_test, axis=1)  # Convert one-hot ground truth to class labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2476a0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.65%\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(Y_test_labels, Y_pred_labels)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cde793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
